
---

# Capstone Project: San Francisco Affordable Housing Predictive Modeling

by Joey Notaro

---

## Overview

The series of notebooks, data files, data visual images, and slideshow presentation contained herein include a review of San Francisco's development pipeline data over the last five years with specific organizational recommendations geared to the San Francisco Mayor's Office of Housing and Community Development (MOHCD). The capstone task of finding a dataset, creating a problem statement, and then modeling to answer the problem statement was generated by General Assembly (GA), but all other data cleaning, exploratory data analysis, modeling, and findings and recommendations represent the expertise and opinions of the author. The following README file contains an executive summary of the problem statement, explanation of data sources and data collection methods, summary of analysis and modeling methodology, and recommendations presented before the MOHCD.

---

## Table of Contents

Readers of this report will find that all notebooks are located in the code directory and numbered in order. Readers may find notebooks should be read in the following order:

1. 01_problem_statement
2. 02_data_cleaning
3. 03_eda
4. 04_clustering_model
5. 05_time_series_model
6. 06_findings_recommendations

---

## Software Required

For reading and interacting with these notebooks and files the following software are recommended for installation by the reader:

* Pandas
* Matplotlib.pyplot
* Seaborn
* Sklearn
* Sktime
* Statsmodel

---

## Problem Statement

The San Francisco Mayor's Office of Housing and Community Development (MOHCD) is concerned with the pace of construction of new housing and particularly affordable housing in the city. The MOHCD is trying to identify which kinds of development projects have been greenlighted by the San Francisco Planning Department in recent years to ascertain:

* What traits or characteristics do large housing unit, especially large affordable housing unit, projects have in common to promote for future development?

* Can we use knowledge of recent past construction numbers to predict future construction trends and make reasonable projections of whether the city is on track to keep up with affordable housing targets?

As the data scientist in charge, we will judge our success on the first question using a clustering model. This clustering model should have a silhouette score of at least 0.75 to sufficiently see distinct enough clusters for us to analyze. Furthermore, we should be able to name some key trends of which clusters have the largest amount of net affordable unit construction and what other key features they might have in common.

Regarding the second question, we will use a litany of time series models and judge success based on the root mean squared error as our evaluation tool. Any successful time series model will possess a root mean squared error at least 50% less than our null model's performance.

---

## Data Collection, Dataset, and Data Dictionary

MOHCD collects contemporary affordable housing data but does not have historic data to look at long-term trends or make predictions. The SF Planning Department collects more robust data on all development projects in the pipeline for the whole city and releases development pipeline data quarterly. Development pipeline data are publicly available stretching back more than 10 years on the [DataSF website](https://datasf.org/opendata/). Despite the Planning Department publishing a relatively consistent data dictionary for the last five years, data features and variable naming conventions have often changed year to year and sometimes quarter to quarter. The author had to compare the published data dictionary against the actual data collected during the cleaning process. The reader may find the data dictionaries dated from 2017, quarter 4, and 2022, quarter 1 in the data_dictionaries directory in this repository. The authoer has made note extensively in Part 2's data cleaning notebook of any deviations from the data dictionary taken during the data cleaning process and the rationale why. Ultimately, five years of quarterly data were collected and cleaned for analysis and modeling to answer our problem statement.

---

## Summary of Modeling Process

In modeling with the data, the following methodology was taken:

1. Clustering Model:

* After data cleaning, 51 features available for feature selection.
* Maintain most salient housing features like net units, affordable units, affordability targets, etc.
* Iterate through numeric features and categorical features separately for feature selection.
* Combine features from both iterations into final production model with goal of 0.75 silhouette score.

2. Time Series Models:

* Focus on forecasting net units and net affordable units.
* Two different time series models for each: Mean monthly new construction **AND** Construction in the pipeline quarterly
* Establish naive last or historical mean null model.
* Iterate through simple exponential smoothing, Holt Winters, and ARIMA models.
* Train vector autoregression for net units and net affordable units, if appropriate.

---

## Organizational Recommendations

Our clustering production model fell far short of target metric for success as did our time series model. Based on the data cleaning process and exploratory data analysis, we recommend the following to MOHCD:

* Work with other city departments to urge the Mayor's Office to hire a data engineer or data architect.
* Find ways to attract more large-scale, mixed residential developments with clear affordability targets.
* Work with the Mayor's office to expedite affordable housing projects through building inspection process.
* Staff in MOHCD need to gather more qualitative research on large-scale outliers and high proportion affordable housing clusters.
* MOHCD cannot adequately predict future affordable housing. It is in the best interest of the Office to focus its policy efforts on increasing the number of new affordable housing projects pushed expediently through the pipeline.